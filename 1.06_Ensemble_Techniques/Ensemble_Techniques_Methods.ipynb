{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ensemble Techniques Methods**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What are Ensemble Methods?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods are techniques that aim at improving the accuracy of results in models by combining multiple models instead of using a single model. The combined models increase the accuracy of the results significantly. This has boosted the popularity of ensemble methods in machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensemble-methods.png](Images/ensemble-methods.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "> - Ensemble methods aim at improving predictability in models by combining several models to make one very reliable model.\n",
    "> - The most popular ensemble methods are boosting, bagging, and stacking.\n",
    "> - Ensemble methods are ideal for regression and classification, where they reduce bias and variance to boost the accuracy of models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Main Types of Ensemble Methods**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Bagging (Bootstrap Aggregating)**\n",
    "\n",
    "- **Concept**: Bagging involves creating multiple subsets of the original dataset through a process called bootstrapping. Bootstrapping randomly samples the data with replacement, creating diverse training sets for each model. Then, individual models (often decision trees) are trained on these subsets in parallel. The final prediction is obtained by averaging (for regression) or voting (for classification) the predictions of all individual models.\n",
    "- **Benefits** The primary benefits of bagging include reducing overfitting, improving stability, and increasing accuracy. The diversity of training sets prevents models from relying on the same patterns, leading to better generalization.\n",
    "\n",
    "- **Example**: Random Forest is a popular bagging algorithm that builds a collection of decision trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bagging_(Bootstrap-Aggregating).png](Images/Bagging_Bootstrap-Aggregating.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Boosting**\n",
    "\n",
    "- **Concept**: Boosting focuses on training models sequentially, where each new model attempts to correct the errors made by the previous ones. The models are weighted based on their performance, and the final prediction is a weighted sum of the predictions from all models.\n",
    "- **Examples**:\n",
    "  - **AdaBoost**: Adjusts the weights of incorrectly classified instances, giving them more importance in the next iteration.\n",
    "  - **Gradient Boosting**: Builds models sequentially, each new model tries to reduce the residual error from the previous model. XGBoost and LightGBM are advanced implementations of this concept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Boosting.png](Images/Boosting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Stacking (Stacked Generalization)**\n",
    "\n",
    "- **Concept**: Stacking involves training multiple base models and then using their predictions as input features to a meta-model (a higher-level model), which makes the final prediction. Unlike bagging and boosting, stacking allows combining different types of models.\n",
    "- **Example**: Combining a logistic regression model, a decision tree, and a support vector machine, with a meta-model like a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stacking(Stacked-Generalization).png](Images/Stacking_Stacked-Generalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bagging_Boosting_Stacking.png](Images/Bagging_Boosting_Stacking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Advantages of Ensemble Methods:**\n",
    "\n",
    "- **Improved Accuracy**: By combining the predictions of multiple models, ensemble methods often result in better performance.\n",
    "- **Reduced Overfitting**: Especially in bagging, the variance of the predictions is reduced, which helps in lowering the risk of overfitting.\n",
    "- **Robustness**: Ensemble methods are typically more robust to outliers and noise in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Disadvantages of Ensemble Methods:**\n",
    "\n",
    "- **Complexity**: They can be more computationally expensive and complex to implement, especially when combining different types of models.\n",
    "- **Interpretability**: The resulting model is often less interpretable compared to a single model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
