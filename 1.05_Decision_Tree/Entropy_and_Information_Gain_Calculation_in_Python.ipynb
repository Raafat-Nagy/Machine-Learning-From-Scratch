{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculating Entropy, Weighted Entropy, and Information Gain for Decision Tree Splitting in Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Entropy**\n",
    "\n",
    "$$\n",
    "\\text{Entropy}(S) = - \\sum_{i=1}^n p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $S$ is the current dataset.\n",
    "- $p_i$ is the proportion of instances in class $i$ relative to the total number of instances.\n",
    "- $n$ is the number of different classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(class_counts: list[int]) -> float:\n",
    "    total_instances = sum(class_counts)\n",
    "    entropy = 0\n",
    "    for count in class_counts:\n",
    "        if count > 0:  # Avoid log(0)\n",
    "            probability = count / total_instances\n",
    "            entropy -= probability * np.log2(probability)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 0.9403\n"
     ]
    }
   ],
   "source": [
    "class_counts = [9, 5]  # 9 instances of class Yes, 5 instances of class No\n",
    "entropy = calculate_entropy(class_counts)\n",
    "print(f\"Entropy: {entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Average Information (Weighted Entropy)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Average Information}(S, A) = \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\times \\text{Entropy}(S_v)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $S$ is the original dataset.\n",
    "- $A$ is the attribute being considered for the split.\n",
    "- $S_v$ is the subset of $S$ for which attribute $A$ has value $v$.\n",
    "- $|S_v|$ is the number of elements in subset $S_v$.\n",
    "- $|S|$ is the total number of elements in the original set $S$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_entropy(subsets: list[list[int]]) -> float:\n",
    "    total_instances = np.sum(subsets)\n",
    "    weighted_entropy = 0\n",
    "\n",
    "    for subset in subsets:\n",
    "        subset_entropy = calculate_entropy(subset)\n",
    "        weighted_entropy += (sum(subset) / total_instances) * subset_entropy\n",
    "    return weighted_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Entropy: 0.9389\n"
     ]
    }
   ],
   "source": [
    "subsets = [[4, 2], [5, 3]]  # Subsets after split: [4 Yes, 2 No] and [5 Yes, 3 No]\n",
    "weighted_entropy = calculate_weighted_entropy(subsets)\n",
    "print(f\"Weighted Entropy: {weighted_entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Information Gain**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Information Gain}(S, A) = \\text{Entropy}(S) - \\text{Average Information}(S, A)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $S$ is the original dataset.\n",
    "- $A$ is the attribute on which the split is based.\n",
    "- $\\text{Entropy}(S)$ is the entropy of the original dataset.\n",
    "- $\\text{Average Information}(S, A)$ is the weighted entropy after splitting.\n",
    "\n",
    "> Take Max Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_gain(\n",
    "    entropy_before_split: float, subsets: list[list[int]]\n",
    ") -> float:\n",
    "    weighted_entropy = calculate_weighted_entropy(subsets)\n",
    "    information_gain = entropy_before_split - weighted_entropy\n",
    "    return information_gain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain: 0.0013\n"
     ]
    }
   ],
   "source": [
    "entropy_before_split = calculate_entropy(class_counts)\n",
    "information_gain = calculate_information_gain(entropy_before_split, subsets)\n",
    "print(f\"Information Gain: {information_gain:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Entropy** measures the impurity of a dataset.\n",
    "- **Average Information (Weighted Entropy)** considers the impurity of the subsets formed after a split.\n",
    "- **Information Gain** measures the reduction in impurity due to the split, helping to select the best attribute for splitting at each step in building a decision tree.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
